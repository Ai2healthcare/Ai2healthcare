---
layout: post
title: "Vision Transformer and its Applications"
author: Dr. Momiao Xiong, Houston, Texas, USA
date: 2023-04-13
categories: news
image: images/Momiao_Xiong.jpg
tags: AI, Deep Learning;Vision;Transformer
---

- Title：MAIB-class-012:Vision Transformer and its Applications
- Date：10:00pm US East time, 04/015/2023
- Date：10:00am Beijing time, 04/16/2023
- Zoom  ID：933 1613 9423
- Zoom PWD：416262
- Zoom: [https://uwmadison.zoom.us/meeting/register/tJcudu-prTIuGNda1MsF8PKyRQlnGn06TP2E](https://uwmadison.zoom.us/meeting/register/tJcudu-prTIuGNda1MsF8PKyRQlnGn06TP2E)

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/hKpz3rairr0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>

* Momiao Xiong, Ph. D, Professor in Department of Biostatistics snd Data Science , University of Texas, School of Public Health. Dr. Xiong graduated from the Department of Statistics at the University of Georgia in 1993. From 1993 to 1995, Dr. Xiong was postdoctoral fellow at the University of Southern California working with Michael Waterman.

* Research Interest： Causal Inference, Artificial Intelligence , Manifold Learning, Statistic Genetics and Bioinformatics .

* [https://theaisummer.com/diffusion-models/#:~:text=Diffusion%20models%20are%20a%20new,to%20train%20large%2Dscale%20models](https://theaisummer.com/diffusion-models/#:~:text=Diffusion%20models%20are%20a%20new,to%20train%20large%2Dscale%20models)

* [https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b](https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b)

* [https://medium.com/red-buffer/implementation-and-understanding-of-graph-neural-networks-gnn-54084c8a0e24](https://medium.com/red-buffer/implementation-and-understanding-of-graph-neural-networks-gnn-54084c8a0e24)

Background

变换器是人工智能在近五年内的最为重要的突破，它在自然语言处理中起了核心的作用。变换器由编码，解码两部分组成。它与自编码器的主要不同之处有以下几点。（1）变换器的编码和解码都是一类特殊的神经网络。它们是由多头，多层次的Self Attention网络组成。编码和解码器的输出都包括了序列数据之间最复杂和最本质的相互作用关系。由他们的输出（即embedding)所形成的空间比变分自编码器隐式高斯空间更为广泛，表达力更强，因而予报精度更高。（2）变换器更容易用作生成器，可生成代码，数学公式，甚至用于推理，计划和决策等。(3)易于推广到图象，视频等数据分析。

变换器在最近这几年也从自然语言处理推广到图象，音频和视频的数据分析中。因为标准的变换器，其计算复杂性是二次。图象，音频和视频都是高维数据，标准的变换器在这些数据中的应用需要巨大的计算量，因此只有改进，才能适应在图象，视频等数据分析的需要。这就导致许多不同形式的变换器。我们就要分几次介绍变换器。介绍它们在分类，描述图象，视频，图象切割，检查产品的质量，诊断疾病，寻找癌组织的位置中的应用。.
