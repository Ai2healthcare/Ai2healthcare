---
layout: post
title: "AIML007： Artificial Intelligence in the Era of Gigantic Pretrained Models"
author: Dr. Boyang Albert Li 
date: 2023-02-02
image: images/Boyang_Li.jpg
categories: news
tags: large language models; GPT-3; visual question answering; prompt tuning
---

- Speaker: Dr. Boyang Albert Li, Nanyang Technological University - NTU Singapore
- Title： MAIB讲座第7期: Artificial Intelligence in the Era of Gigantic Pretrained Models
- Date：9:00pm US East time, 01/28/2023
- Date：10:00am Beijing time, 01/29/2023
- Zoom  ID： 933 1613 9423
- Zoom PWD： 416262
- Key words: large language models; GPT-3; visual question answering; prompt tuning

Title: Artificial Intelligence in the Era of Gigantic Pretrained Models

自然语言模型是生成式人工智能的又一重要方法。扩散模型源于连续变量，而自然语言模型源于离散变量。自然语言模型的重要技术支柱之一是自注意力。现在连续，离散，Transformer和自然语言的自回归正在相互融合，成为人工智能的主战場之一，在我们的推理，智能活动，生物学，健康卫生和医学中都有广泛的应用。OpenAI 继推出chat GPT 之后又在模拟人的智力活动和思维过程。上述领域是我们末来几个月活动的主要内容。由于知识涉及面广，成百上千的文章湧现，我们都来不及读。但我们会尽力尽量选取有代表性的论文介绍。在此之后，我们讨论因果分析和人工智能。然后，我们介绍微分流形的基本知识和在人工智能中的应用。王一老师这周的个性化自然语言的报告，是我们这个另一生成式人工智能活动的开始。我们认为，这是走向一般人工智能的途径之一。

Abstract:

2020年GPT-3的实现，标志着人工智能的研究进入了超大规模预训练模型的时代。超大规模模型带来是一系列之前没人预见到的能力，如In-context learning 以及与之相适应的各种Prompt技巧。但同时超大规模模型也在模型部署和训练等方面带来了前所未有的挑战。在本次讲座中，我们将重点回顾最具影响力的GPT-3 系列模型的历史（GPT-3, InstructGPT, ChatGPT） 和他们使用的技术，它们的性能特点，优势，以及现有的不足。在此基础上，我们将讨论如何有效利用这些模型，在不经过任何训练的情况下，可以进行视觉问答问题的求解 [1, 2]，以及如何（部分地）解决模型部署的痛点 [3,4]。

Bio:

Boyang Albert Li 现任南洋理工大学（Nanyang Technological University - NTU Singapore）南洋副教授。2021年他获得新加坡国家科研基金委授予的NRF Fellow荣誉及两百五十万新加坡元的科研基金。他在2018-2019年间任职于百度美国研究院，在2015-2017年间任职于迪士尼匹兹堡研究院并领导独立研究小组。2015年，他自佐治亚理工学院（Georgia Institute of Technology）取得博士学位。2008年他自南洋理工大学取得学士学位。他的工作曾被多家国际媒体报道，其中包括英国卫报和《新科学人》杂志，美国全国公共广播电台（National Public Radio）, Engadget, TechCrunch，日本Web担当者Forum等媒体。

* [1] Anthony Meng Huat Tiong, Junnan Li, Boyang Li, Silvio Savarese, Steven CH Hoi. Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training. EMNLP Findings. 2022.
* [2] Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, Steven CH Hoi. From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models. Arxiv 2212.10846
* [3] Xu Guo, Boyang Li, Han Yu. Improving the Sample Efficiency of Prompt Tuning with Domain Adaptation. EMNLP Findings. 2022.
* [4] Bosheng Ding, Chengwei Qin, Linlin Liu, Lidong Bing, Shafiq Joty, Boyang Li. Is GPT-3 a Good Data Annotator? Arxiv 2212.10450

<p align="center">
<iframe width="560" height="315" src="https://www.youtube.com/embed/U_1yMszMzBA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>


HealthScienceHub, [https://space.bilibili.com/2056525058](https://space.bilibili.com/2056525058)


<p align="center">
<iframe width="560" height="315" src="https://www.bilibili.com/video/BV11d4y1W7EC/?share_source=copy_web&vd_source=28eb47824962ef1aab68d1506a52b55c" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</p>

